{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c9ad4c-81d1-41af-84fa-72e2d62e7ee1",
   "metadata": {},
   "source": [
    "### Ensemble Learning Introduction\n",
    "\n",
    "> Suppose you pose a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus, this technique is called ensemble learning, and an ensemble learning algorithm is called an ensemble method.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Such an ensemble of decision trees is called a random forest, and despite its simplicity, this is one of the most powerful machine learning algorithms available today.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "### Voting Classifiers\n",
    "> Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a logistic regression classifier, an SVM classifier, a random forest classifier, a k-nearest neighbors classifier, and perhaps a few more (see Figure 7-1).\n",
    "> ...\n",
    "> A very simple way to create an even better classifier is to aggregate the predictions of each classifier: the class that gets the most votes is the ensemble’s prediction. This majority-vote classifier is called a hard voting classifier\n",
    "> ...\n",
    "> this voting classifier often achieves a higher accuracy than the best classifier in the ensemble\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Similarly, suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy!\n",
    "> ...\n",
    "> However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Scikit-Learn provides a VotingClassifier class that’s quite easy to use: just give it a list of name/predictor pairs, and use it like a normal classifier.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557f33f-581b-4f48-87b0-57ca3054c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code demonstrates how to use Sklearn's built-in VotingClassifier to aggregate the results of a series of classifiers to\n",
    "# provide a high-accuracy result\n",
    "\n",
    "# Make two interleaving half circles\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# RandomForestClassifier\n",
    "# A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of\n",
    "# the dataset and uses averaging to improve the predictive accuracy and control over-fitting\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "#\n",
    "# VotingClassifier\n",
    "# Soft Voting/Majority Rule classifier for unfitted estimators.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# LogisticRegression\n",
    "# Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# train_test_split\n",
    "# Split arrays or matrices into random train and test subsets.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVC\n",
    "# C-Support Vector Classification.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate the data set\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "\n",
    "# Categorize the data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create the classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# View each classifier's estimations \n",
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))\n",
    "\n",
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e4b45-bf16-48aa-8459-c79da85fda91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998c83f-84d7-43b5-aa6f-ee322a194870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
