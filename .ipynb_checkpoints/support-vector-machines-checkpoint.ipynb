{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2609139-006f-45b5-a087-24123bf858fd",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "> A support vector machine (SVM) is a powerful and versatile machine learning model, capable of performing linear or nonlinear classification, regression, and even novelty detection\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "> Does not scale very well to very large datasets.\n",
    "> > Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "> The decision boundary of an SVM classifier....(is a)...line...(that)...not only separates...two classes but also stays as far away from the closest training instances as possible.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "> The best hyperplane, also known as the “hard margin,” is the one that maximizes the distance between the hyperplane and the nearest data points from both classes.\n",
    "> https://www.geeksforgeeks.org/support-vector-machine-algorithm/\n",
    "\n",
    "> Adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the support vectors.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "### Soft and Hard Margin Classification\n",
    "> If we strictly impose that all instances must be off the street and on the correct side, this is called hard margin classification. There are two main issues with hard margin classification. First, it only works if the data is linearly separable. Second, it is sensitive to outliers.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "> The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). This is called soft margin classification.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "> Regularization hyperparameters, such as the regularization parameter in linear regression or the dropout rate in neural networks, control the model's complexity. Higher values of these hyperparameters penalize complex models, helping to prevent overfitting.\n",
    "> https://encord.com/glossary/hyper-parameters-definition/#:~:text=Regularization%20hyperparameters%2C%20such%20as%20the,models%2C%20helping%20to%20prevent%20overfitting.\n",
    "\n",
    "> If you set it to a low value, then you end up with the model on the left of Figure 5-4. With a high value, you get the model on the right. As you can see, reducing C makes the street larger, but it also leads to more margin violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3092bec8-d79d-4ffc-93d6-203d5d1a8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data':      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns], 'target': 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int64, 'frame':      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "     target  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "..      ...  \n",
      "145       2  \n",
      "146       2  \n",
      "147       2  \n",
      "148       2  \n",
      "149       2  \n",
      "\n",
      "[150 rows x 5 columns], 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. dropdown:: References\\n\\n  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n    Mathematical Statistics\" (John Wiley, NY, 1950).\\n  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n    Structure and Classification Rule for Recognition in Partially Exposed\\n    Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n    Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n    on Information Theory, May 1972, 431-433.\\n  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n    conceptual clustering system finds 3 classes in the data.\\n  - Many, many more ...\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n",
      "[ True False]\n",
      "[ 0.66163816 -0.22035761]\n"
     ]
    }
   ],
   "source": [
    "# The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect Iris virginica flowers.\n",
    "# The pipeline first scales the features, then uses a LinearSVC with C=1:\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Load the dataset as a pandas dataframe (i.e. two-dimensional data structure)\n",
    "iris = load_iris(as_frame=True)\n",
    "print(iris)\n",
    "\n",
    "# Sets X to a two-dimensional array, with each index containing an array of the values for\n",
    "# petal length (cm), petal width (cm)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "# print(X)\n",
    "# print(X[0][0])\n",
    "# print(X[0][1])\n",
    "\n",
    "# Sets Y to the iris.target of the desired value for the classifier --> # Iris virginica\n",
    "y = (iris.target == 2)\n",
    "\n",
    "# Create the classifier and apply it to the extracted dataset\n",
    "svm_clf = make_pipeline(StandardScaler(), LinearSVC(C=1, random_state=42))\n",
    "svm_clf.fit(X, y)\n",
    "# make_pipeline: Construct a pipeline\n",
    "# Pipeline allows you to sequentially apply a list of transformers to preprocess the data and,\n",
    "# if desired, conclude the sequence with a final predictor for predictive modeling.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n",
    "\n",
    "# StandardScaler(): Standardize features by removing the mean and scaling to unit variance\n",
    "# https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "# LinearSVC(C=1, random_state=42)): Linear Support Vector Classification\n",
    "# https://scikit-learn.org/1.6/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "# Use the model to make a prediction\n",
    "X_new = [[5.5, 1.7], [5.0, 1.5]]\n",
    "print(svm_clf.predict(X_new))\n",
    "\n",
    "# Display the signed distance between each instance and the decision boundary\n",
    "print(svm_clf.decision_function(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be22b4a-dbe6-4367-b218-940b6186a54c",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Classification\n",
    "\n",
    "> Although linear SVM classifiers are efficient and often work surprisingly well, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features (as we did in Chapter 4); in some cases this can result in a linearly separable dataset.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "To implement this idea using Scikit-Learn, you can create a pipeline containing a PolynomialFeatures transformer (discussed in “Polynomial Regression”), followed by a StandardScaler and a LinearSVC classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed40427c-a1c3-40e0-b119-3888bb0e3f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=3)),\n",
      "                ('standardscaler', StandardScaler()),\n",
      "                ('linearsvc',\n",
      "                 LinearSVC(C=10, max_iter=10000, random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# The following Scikit-Learn code generates a moon dataset and trains a polynomial SVM classifier\n",
    "# The pipeline first introduces polynomial features, then scales the data set, and finally applies\n",
    "# Linear SVM\n",
    "\n",
    "# Construct a Pipeline from the given estimators.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate a new feature matrix consisting of all polynomial combinations of\n",
    "# the features with degree less than or equal to the specified degree. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Linear Support Vector Classification\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Make two interleaving half circles.\n",
    "# A simple toy dataset to visualize clustering and classification algorithms.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# n - the total number of points generated \n",
    "# noise - Standard deviation of Gaussian noise added to the data\n",
    "# random_state - Determines random number generation for dataset shuffling and noise.\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "# Creates the classfier and applies it to the moon data set\n",
    "# C - Regularization parameter - The regularization parameter in a\n",
    "# Support Vector Machine (SVM) is \\(C\\),which controls how much to penalize misclassified data. \n",
    "# Google AI\n",
    "# max_iter - The maximum number of iterations to be run\n",
    "# random_state - Controls the pseudo random number generation for shuffling the data \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "polynomial_svm_clf = make_pipeline(\n",
    "    PolynomialFeatures(degree=3),\n",
    "    StandardScaler(),\n",
    "    LinearSVC(C=10, max_iter=10_000, random_state=42)\n",
    ")\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0698bf-4f26-428c-a5b6-6a48b2efa4ce",
   "metadata": {},
   "source": [
    "# Polynomial Kernel\n",
    "\n",
    "> Adding polynomial features is simple to implement and can work great with all sorts of machine learning algorithms (not just SVMs). That said, at a low polynomial degree this method cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "> The kernel trick makes it possible to get the same result as if you had added many polynomial features, even with a very high degree, without actually having to add them.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition - Chapter 5\n",
    "\n",
    "> Although there are some obstacles to understanding the kernel trick, it is highly important to understand how kernels are used in support vector classification. For practical reasons, it is important to understand because implementing support vector classifiers requires specifying a kernel function, and there are not established, general rules to know what kernel will work best for your particular data.\n",
    "> https://medium.com/towards-data-science/the-kernel-trick-c98cdbcaeb3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc2ee7-83dc-4472-a698-a9f181bea6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038e2a3-aae7-47bd-a8df-4921a74e148a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
