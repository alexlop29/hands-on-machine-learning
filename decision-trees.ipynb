{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f9e72b-39b9-4dc4-b579-dc12165a3c1a",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "> Decision trees are versatile machine learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Decision trees are also the fundamental components of random forests (see Chapter 7), which are among the most powerful machine learning algorithms available today.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> How Decision Trees Work?\n",
    "> A decision tree working starts with a main question known as the root node. This question is derived from the features of the dataset and serves as the starting point for decision-making.\n",
    "> From the root node, the tree asks a series of yes/no questions. Each question is designed to split the data into subsets based on specific attributes.\n",
    "> https://www.geeksforgeeks.org/decision-tree/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ed9705-8999-4e9a-9ce1-cab602d5ae97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.0 (20241103.1931)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"350pt\" height=\"314pt\"\n",
       " viewBox=\"0.00 0.00 349.50 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-310 345.5,-310 345.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M208,-306C208,-306 64.5,-306 64.5,-306 58.5,-306 52.5,-300 52.5,-294 52.5,-294 52.5,-235 52.5,-235 52.5,-229 58.5,-223 64.5,-223 64.5,-223 208,-223 208,-223 214,-223 220,-229 220,-235 220,-235 220,-294 220,-294 220,-300 214,-306 208,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"136.25\" y=\"-288.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) &lt;= 2.45</text>\n",
       "<text text-anchor=\"middle\" x=\"136.25\" y=\"-273.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n",
       "<text text-anchor=\"middle\" x=\"136.25\" y=\"-258.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n",
       "<text text-anchor=\"middle\" x=\"136.25\" y=\"-243.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n",
       "<text text-anchor=\"middle\" x=\"136.25\" y=\"-228.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M104.5,-179.5C104.5,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 104.5,-111.5 104.5,-111.5 110.5,-111.5 116.5,-117.5 116.5,-123.5 116.5,-123.5 116.5,-167.5 116.5,-167.5 116.5,-173.5 110.5,-179.5 104.5,-179.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"58.25\" y=\"-162.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"58.25\" y=\"-147.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"middle\" x=\"58.25\" y=\"-132.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n",
       "<text text-anchor=\"middle\" x=\"58.25\" y=\"-117.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M108.97,-222.58C101.77,-211.77 93.97,-200.09 86.71,-189.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.79,-187.5 81.33,-181.12 83.97,-191.38 89.79,-187.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.59\" y=\"-198.58\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M284.38,-187C284.38,-187 146.12,-187 146.12,-187 140.12,-187 134.12,-181 134.12,-175 134.12,-175 134.12,-116 134.12,-116 134.12,-110 140.12,-104 146.12,-104 146.12,-104 284.38,-104 284.38,-104 290.38,-104 296.38,-110 296.38,-116 296.38,-116 296.38,-175 296.38,-175 296.38,-181 290.38,-187 284.38,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.25\" y=\"-169.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.75</text>\n",
       "<text text-anchor=\"middle\" x=\"215.25\" y=\"-154.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"215.25\" y=\"-139.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"215.25\" y=\"-124.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n",
       "<text text-anchor=\"middle\" x=\"215.25\" y=\"-109.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.88,-222.58C169.57,-214.16 175.61,-205.2 181.49,-196.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"184.23,-198.7 186.92,-188.45 178.42,-194.78 184.23,-198.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"192.53\" y=\"-205.94\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#4de88e\" stroke=\"black\" d=\"M195.38,-68C195.38,-68 99.12,-68 99.12,-68 93.12,-68 87.12,-62 87.12,-56 87.12,-56 87.12,-12 87.12,-12 87.12,-6 93.12,0 99.12,0 99.12,0 195.38,0 195.38,0 201.38,0 207.38,-6 207.38,-12 207.38,-12 207.38,-56 207.38,-56 207.38,-62 201.38,-68 195.38,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"147.25\" y=\"-50.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n",
       "<text text-anchor=\"middle\" x=\"147.25\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n",
       "<text text-anchor=\"middle\" x=\"147.25\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n",
       "<text text-anchor=\"middle\" x=\"147.25\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.93,-103.73C184.72,-95.34 179.22,-86.47 173.96,-78.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176.98,-76.23 168.73,-69.59 171.03,-79.93 176.98,-76.23\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#843de6\" stroke=\"black\" d=\"M329.5,-68C329.5,-68 237,-68 237,-68 231,-68 225,-62 225,-56 225,-56 225,-12 225,-12 225,-6 231,0 237,0 237,0 329.5,0 329.5,0 335.5,0 341.5,-6 341.5,-12 341.5,-12 341.5,-56 341.5,-56 341.5,-62 335.5,-68 329.5,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.25\" y=\"-50.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n",
       "<text text-anchor=\"middle\" x=\"283.25\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"middle\" x=\"283.25\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n",
       "<text text-anchor=\"middle\" x=\"283.25\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M240.57,-103.73C245.78,-95.34 251.28,-86.47 256.54,-78.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"259.47,-79.93 261.77,-69.59 253.52,-76.23 259.47,-79.93\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x177429a90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code demonstrates building a decision tree classifier\n",
    "\n",
    "# Load and return the iris dataset (classification).\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# A decision tree classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Export a decision tree in DOT format\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Load and display files\n",
    "from graphviz import Source\n",
    "\n",
    "# Load the data set\n",
    "iris = load_iris(as_frame=True)\n",
    "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y_iris = iris.target\n",
    "\n",
    "# Train the classifier and fit the data\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "Source.from_file(\"iris_tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b136e-cae3-45c8-bbe7-233458a0255d",
   "metadata": {},
   "source": [
    "> Graphviz is an open source graph visualization software package. It also includes a dot command-line tool to convert .dot files to a variety of formats, such as PDF or PNG.\n",
    "> TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> One of the many qualities of decision trees is that they require very little data preparation. In fact, they don’t require feature scaling or centering at all.\n",
    "> TensorFlow, 3rd Edition Aurélien Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5ab84-bf0f-4f42-ad4e-e9a753b2561d",
   "metadata": {},
   "source": [
    "> A node’s samples attribute counts how many training instances it applies to.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> A node’s value attribute tells you how many training instances of each class this node applies to: for example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor, and 45 Iris virginica.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Finally, a node’s gini attribute measures its Gini impurity: a node is “pure” (gini=0) if all training instances it applies to belong to the same class.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "(Seemingly refers to an instance whose values do not conform to the expectations of its class?)\n",
    "\n",
    "> Scikit-Learn uses the CART algorithm, which produces only binary trees, meaning trees where split nodes always have exactly two children (i.e., questions only have yes/no answers). However, other algorithms, such as ID3, can produce decision trees with nodes that have more than two children.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Decision trees are intuitive, and their decisions are easy to interpret. Such models are often called white box models. In contrast, as you will see, random forests and neural networks are generally considered black box models.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Black box models make great predictions, and you can easily check the calculations that they performed to make these predictions; nevertheless, it is usually hard to explain in simple terms why the predictions were made.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Conversely, decision trees provide nice, simple classification rules that can even be applied manually if need be (e.g., for flower classification).\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> A decision tree can also estimate the probability that an instance belongs to a particular class k. First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f19c659-885b-42b2-a84f-a7603ff63527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]\n",
      " [3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]\n",
      " [4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]\n",
      " [5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n",
      "[[0.    0.907 0.093]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# The following code demonstrates building a decision tree classifier\n",
    "\n",
    "# Load and return the iris dataset (classification).\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# A decision tree classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Export a decision tree in DOT format\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# Load and display files\n",
    "from graphviz import Source\n",
    "\n",
    "# Load the data set\n",
    "iris = load_iris(as_frame=True)\n",
    "print(iris.DESCR)\n",
    "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "print(X_iris)\n",
    "y_iris = iris.target\n",
    "\n",
    "# Train the classifier and fit the data\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "Source.from_file(\"iris_tree.dot\")\n",
    "\n",
    "# Estimate the probability that an instance belongs to a particulare class k\n",
    "# Predict class probabilities of the input samples X.\n",
    "# The predicted class probability is the fraction of samples of the same class in a leaf.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict_proba\n",
    "print(tree_clf.predict_proba([[5, 1.5]]).round(3))\n",
    "print(tree_clf.predict([[5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2228b8a-14ff-4b98-b65b-698d84a7b25a",
   "metadata": {},
   "source": [
    "### The CART Training Algorithm\n",
    "\n",
    "> Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train decision trees (also called “growing” trees). The algorithm works by first splitting the training set into two subsets using a single feature k and a threshold tk (e.g., “petal length ≤ 2.45 cm”). How does it choose k and tk? It searches for the pair (k, tk) that produces the purest subsets, weighted by their size.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> As you can see, the CART algorithm is a greedy algorithm: it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> A greedy algorithm often produces a solution that’s reasonably good but not guaranteed to be optimal.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "Need refresher on greedy algorithms and time/space complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8a939-87d1-406a-b860-29e181ff40eb",
   "metadata": {},
   "source": [
    "### Gini Impurity or Entropy\n",
    "\n",
    "> By default, the DecisionTreeClassifier class uses the Gini impurity measure, but you can select the entropy impurity measure instead by setting the criterion hyperparameter to \"entropy\".\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> Entropy is zero when all messages are identical. In machine learning, entropy is frequently used as an impurity measure: a set’s entropy is zero when it contains instances of only one class.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron\n",
    "\n",
    "> The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.\n",
    "> Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition Aurélien Géron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607e6b47-cf4d-4977-a2cb-8371559d4a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8666666666666666\n"
     ]
    }
   ],
   "source": [
    "# The following code demonstrates building a decision tree with the make moons data set\n",
    "\n",
    "# Make two interleaving half circles.\n",
    "# A simple toy dataset to visualize clustering and classification algorithms.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Split arrays or matrices into random train and test subsets\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# A decision tree classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Evaluate a score by cross-validation.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# n - the total number of points generated \n",
    "# noise - Standard deviation of Gaussian noise added to the data\n",
    "# random_state - Determines random number generation for dataset shuffling and noise.\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Train the classifier and fit the data\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Assess the cross validation score\n",
    "print(cross_val_score(tree_clf, X_train, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b1d33-3a44-4252-a77e-d4d23cad76b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
